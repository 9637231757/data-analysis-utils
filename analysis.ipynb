{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9637231757/data-analysis-utils/blob/main/analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import gzip\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "from collections import Counter\n",
        "import urllib.request\n",
        "import os\n",
        "import sys # Import sys for sys.exit()\n",
        "\n",
        "# Part 1: Data Loading and Cleaning\n",
        "\n",
        "# Download and load the dataset\n",
        "def download_and_load_data():\n",
        "    url = \"ftp://ita.ee.lbl.gov/traces/calgary_access_log.gz\"\n",
        "    local_file = \"calgary_access_log.gz\"\n",
        "\n",
        "    # Download with error handling\n",
        "    try:\n",
        "        if not os.path.exists(local_file):\n",
        "            print(f\"Downloading dataset from {url}...\")\n",
        "            urllib.request.urlretrieve(url, local_file)\n",
        "            print(\"Dataset downloaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Read the compressed file with latin1 encoding to handle non-UTF-8 bytes\n",
        "    try:\n",
        "        print(f\"Reading dataset from {local_file}...\")\n",
        "        with gzip.open(local_file, 'rt', encoding='latin1') as f:\n",
        "            lines = f.readlines()\n",
        "        print(\"Dataset read successfully.\")\n",
        "        return lines\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Local file {local_file} not found after download attempt.\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading dataset: {e}\")\n",
        "        return None\n",
        "\n",
        "# Parse log entries\n",
        "def parse_log_entry(line):\n",
        "    # Apache Common Log Format regex\n",
        "    # Added '?' after * in ([^\"]*) to make it non-greedy for the request part\n",
        "    # Changed request parsing slightly to be more robust\n",
        "    pattern = r'(\\S+) (\\S+) (\\S+) \\[(.*?)\\] \"([^\"]*)\" (\\d+) (\\S+)'\n",
        "    match = re.match(pattern, line)\n",
        "\n",
        "    if not match:\n",
        "        # print(f\"Warning: Line did not match pattern: {line.strip()}\") # Uncomment for debugging unparsed lines\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        host, _, _, timestamp_str, request, http_code, bytes_transferred = match.groups()\n",
        "\n",
        "        # Parse timestamp - handle potential timezone issues gracefully\n",
        "        try:\n",
        "            # Remove the timezone offset before parsing if it exists, or handle it\n",
        "            # The original format string \"%d/%b/%Y:%H:%M:%S %z\" is correct if the timezone is present\n",
        "            # If the timezone is missing or malformed, this will raise ValueError\n",
        "            timestamp = datetime.strptime(timestamp_str, \"%d/%b/%Y:%H:%M:%S %z\")\n",
        "        except ValueError:\n",
        "             # Try parsing without timezone if the above fails\n",
        "            try:\n",
        "                timestamp = datetime.strptime(timestamp_str.split(' ')[0], \"%d/%b/%Y:%H:%M:%S\")\n",
        "                # print(f\"Warning: Timestamp parsed without timezone for line: {line.strip()}\") # Uncomment for debugging\n",
        "            except ValueError:\n",
        "                # print(f\"Warning: Invalid timestamp format: {timestamp_str} in line: {line.strip()}\") # Uncomment for debugging\n",
        "                return None # Return None for truly invalid timestamps\n",
        "\n",
        "        # Extract filename from request\n",
        "        try:\n",
        "            request_parts = request.split()\n",
        "            filename = request_parts[1] if len(request_parts) > 1 else \"\"\n",
        "            # Handle cases like 'GET / HTTP/1.0' where filename is '/'\n",
        "            if filename.startswith('/'):\n",
        "                 filename = filename[1:] # Remove leading slash\n",
        "\n",
        "            # Extract file extension\n",
        "            # Ensure filename is not empty before splitting\n",
        "            file_ext = filename.split('.')[-1].lower() if '.' in filename and filename else 'no_ext'\n",
        "            # Handle cases like '/' which has no extension\n",
        "            if file_ext == filename.lower() and '.' not in filename:\n",
        "                 file_ext = 'no_ext'\n",
        "\n",
        "\n",
        "        except IndexError:\n",
        "            # print(f\"Warning: Could not extract filename from request: {request} in line: {line.strip()}\") # Uncomment for debugging\n",
        "            filename = \"\"\n",
        "            file_ext = 'no_ext'\n",
        "        except Exception as e:\n",
        "             # print(f\"Warning: Unexpected error extracting filename: {e} from request: {request} in line: {line.strip()}\") # Uncomment for debugging\n",
        "             filename = \"\"\n",
        "             file_ext = 'no_ext'\n",
        "\n",
        "\n",
        "        # Handle bytes_transferred\n",
        "        try:\n",
        "            bytes_transferred = int(bytes_transferred) if bytes_transferred != '-' else 0\n",
        "        except ValueError:\n",
        "            # print(f\"Warning: Invalid bytes value: {bytes_transferred} in line: {line.strip()}\") # Uncomment for debugging\n",
        "            bytes_transferred = 0 # Default to 0 if bytes are not a valid number\n",
        "\n",
        "        return {\n",
        "            'host': host,\n",
        "            'timestamp': timestamp,\n",
        "            'filename': filename,\n",
        "            'file_ext': file_ext,\n",
        "            'http_code': int(http_code),\n",
        "            'bytes': bytes_transferred\n",
        "        }\n",
        "    except Exception as e:\n",
        "        # Catch any other unexpected errors during parsing a matched line\n",
        "        # print(f\"Warning: Unexpected error parsing matched line: {e} for line: {line.strip()}\") # Uncomment for debugging\n",
        "        return None\n",
        "\n",
        "\n",
        "# Load and clean data\n",
        "def load_and_clean_data():\n",
        "    lines = download_and_load_data()\n",
        "    if not lines:\n",
        "        print(\"No lines loaded from the dataset.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Parsing log entries...\")\n",
        "    # Parse all log entries and filter out None values\n",
        "    # Use a generator expression and filter for efficiency\n",
        "    parsed_entries_gen = (parse_log_entry(line) for line in lines)\n",
        "    parsed_entries = [entry for entry in parsed_entries_gen if entry is not None]\n",
        "    print(f\"Successfully parsed {len(parsed_entries)} entries out of {len(lines)}.\")\n",
        "\n",
        "    # If no entries parsed successfully, return None\n",
        "    if not parsed_entries:\n",
        "        print(\"Warning: No log entries were successfully parsed after filtering.\")\n",
        "        return None\n",
        "\n",
        "    print(\"Creating DataFrame...\")\n",
        "    try:\n",
        "        df = pd.DataFrame(parsed_entries)\n",
        "        print(\"DataFrame created successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating DataFrame: {e}\")\n",
        "        return None\n",
        "\n",
        "    # The following steps are generally good practice but might be less critical\n",
        "    # now that None entries are filtered before DataFrame creation.\n",
        "    # However, keeping them adds robustness.\n",
        "\n",
        "    # Explicitly convert 'timestamp' to datetime, coercing errors.\n",
        "    # This ensures the column is datetime-like even if parsing failed for some rows.\n",
        "    # Note: With filtering None entries above, this might be redundant but kept for safety\n",
        "    print(\"Converting timestamp column to datetime...\")\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
        "    print(\"Timestamp column conversion complete.\")\n",
        "\n",
        "\n",
        "    # Remove rows where the timestamp could not be parsed (are NaT)\n",
        "    # This step is less likely to remove rows after filtering None entries\n",
        "    initial_rows = len(df)\n",
        "    df.dropna(subset=['timestamp'], inplace=True)\n",
        "    rows_removed_ts_na = initial_rows - len(df)\n",
        "    if rows_removed_ts_na > 0:\n",
        "        print(f\"Removed {rows_removed_ts_na} rows with invalid timestamps after DataFrame creation.\")\n",
        "\n",
        "\n",
        "    # Check if DataFrame is empty after cleaning\n",
        "    if df.empty:\n",
        "        print(\"DataFrame is empty after cleaning.\")\n",
        "        return None\n",
        "\n",
        "    # Now the .dt accessor will work correctly on the 'timestamp' column\n",
        "    # Add a check to ensure 'timestamp' column is indeed datetime before accessing .dt\n",
        "    if pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
        "        print(\"Extracting date and hour from timestamp...\")\n",
        "        df['date'] = df['timestamp'].dt.strftime('%d-%b-%Y')\n",
        "        df['hour'] = df['timestamp'].dt.hour\n",
        "        print(\"Date and hour columns added.\")\n",
        "    else:\n",
        "        print(\"Warning: 'timestamp' column is not datetime type after cleaning. Date/Hour columns skipped.\")\n",
        "        # Optionally, return None or an empty DataFrame if datetime conversion is critical\n",
        "        # return None\n",
        "\n",
        "\n",
        "    return df\n",
        "\n",
        "# Load the data\n",
        "df = load_and_clean_data()\n",
        "\n",
        "# Only proceed to analysis if the DataFrame was successfully loaded\n",
        "if df is None or df.empty:\n",
        "    print(\"Failed to load data or no data available after cleaning. Analysis skipped.\")\n",
        "    # Use sys.exit() for exiting in a script context, but exit() might be sufficient in Jupyter\n",
        "    sys.exit(\"Data loading and cleaning failed.\")\n",
        "else:\n",
        "    print(f\"\\nData loaded and cleaned. DataFrame shape: {df.shape}\")\n",
        "    print(\"DataFrame columns:\", df.columns.tolist())\n",
        "    # print(\"\\nSample data:\") # Uncomment to see sample data\n",
        "    # display(df.head())\n",
        "\n",
        "    # Part 2: Analysis Questions\n",
        "\n",
        "    # Q1: Count of total log records\n",
        "    def q1_total_records(df):\n",
        "        return len(df)\n",
        "\n",
        "    # Q2: Count of unique hosts\n",
        "    def q2_unique_hosts(df):\n",
        "        return df['host'].nunique()\n",
        "\n",
        "    # Q3: Date-wise unique filename counts\n",
        "    def q3_datewise_filenames(df):\n",
        "        # Add a check to ensure 'date' column exists\n",
        "        if 'date' in df.columns:\n",
        "            print(\"Calculating date-wise unique filename counts...\")\n",
        "            result = df.groupby('date')['filename'].nunique().to_dict()\n",
        "            print(\"Calculation complete.\")\n",
        "            return result\n",
        "        else:\n",
        "            print(\"Warning: 'date' column not found for Q3.\")\n",
        "            return {}\n",
        "\n",
        "\n",
        "    # Q4: Number of 404 response codes\n",
        "    def q4_count_404(df):\n",
        "        print(\"Counting 404 responses...\")\n",
        "        count = len(df[df['http_code'] == 404])\n",
        "        print(\"Count complete.\")\n",
        "        return count\n",
        "\n",
        "    # Q5: Top 15 filenames with 404 responses\n",
        "    def q5_top_15_404_filenames(df):\n",
        "        print(\"Finding top 15 filenames with 404 responses...\")\n",
        "        df_404 = df[df['http_code'] == 404]\n",
        "        if not df_404.empty:\n",
        "             result = Counter(df_404['filename']).most_common(15)\n",
        "        else:\n",
        "             result = []\n",
        "             print(\"No 404 responses found.\")\n",
        "        print(\"Calculation complete.\")\n",
        "        return result\n",
        "\n",
        "    # Q6: Top 15 file extensions with 404 responses\n",
        "    def q6_top_15_404_extensions(df):\n",
        "        print(\"Finding top 15 file extensions with 404 responses...\")\n",
        "        df_404 = df[df['http_code'] == 404]\n",
        "        if not df_404.empty:\n",
        "            result = Counter(df_404['file_ext']).most_common(15)\n",
        "        else:\n",
        "             result = []\n",
        "             print(\"No 404 responses found.\")\n",
        "        print(\"Calculation complete.\")\n",
        "        return result\n",
        "\n",
        "\n",
        "    # Q7: Total bandwidth transferred per day for July 1995\n",
        "    def q7_july_bandwidth(df):\n",
        "        print(\"Calculating total bandwidth per day for July 1995...\")\n",
        "        # Add a check to ensure 'timestamp' is datetime type before filtering\n",
        "        if pd.api.types.is_datetime64_any_dtype(df['timestamp']):\n",
        "            # Ensure the timestamp includes the year for accurate filtering\n",
        "            # Filter for entries in July 1995\n",
        "            df_july_1995 = df[(df['timestamp'].dt.year == 1995) & (df['timestamp'].dt.month == 7)]\n",
        "\n",
        "            # Add a check to ensure 'date' column exists in the filtered df and it's not empty\n",
        "            if 'date' in df_july_1995.columns and not df_july_1995.empty:\n",
        "                 result = df_july_1995.groupby('date')['bytes'].sum().to_dict()\n",
        "                 print(\"Calculation complete.\")\n",
        "                 return result\n",
        "            elif 'date' not in df_july_1995.columns:\n",
        "                print(\"Warning: 'date' column not found for Q7 after filtering.\")\n",
        "                return {}\n",
        "            else: # df_july_1995 is empty\n",
        "                 print(\"No entries found for July 1995.\")\n",
        "                 return {}\n",
        "\n",
        "        else:\n",
        "            print(\"Warning: 'timestamp' column is not datetime type for Q7. Calculation skipped.\")\n",
        "            return {}\n",
        "\n",
        "\n",
        "    # Q8: Hourly request distribution\n",
        "    def q8_hourly_distribution(df):\n",
        "        print(\"Calculating hourly request distribution...\")\n",
        "        # Add a check to ensure 'hour' column exists\n",
        "        if 'hour' in df.columns:\n",
        "            result = df['hour'].value_counts().sort_index().to_dict()\n",
        "            print(\"Calculation complete.\")\n",
        "            return result\n",
        "        else:\n",
        "            print(\"Warning: 'hour' column not found for Q8.\")\n",
        "            return {}\n",
        "\n",
        "    # Q9: Top 10 most requested filenames\n",
        "    def q9_top_10_filenames(df):\n",
        "        print(\"Finding top 10 most requested filenames...\")\n",
        "        result = Counter(df['filename']).most_common(10)\n",
        "        print(\"Calculation complete.\")\n",
        "        return result\n",
        "\n",
        "    # Q10: HTTP response code distribution\n",
        "    def q10_http_code_distribution(df):\n",
        "        print(\"Calculating HTTP response code distribution...\")\n",
        "        result = df['http_code'].value_counts().to_dict()\n",
        "        print(\"Calculation complete.\")\n",
        "        return result\n",
        "\n",
        "    # Execute all questions and display results\n",
        "    print(\"\\n--- Analysis Results ---\")\n",
        "    print(\"Q1 - Total log records:\", q1_total_records(df))\n",
        "    print(\"Q2 - Unique hosts:\", q2_unique_hosts(df))\n",
        "    print(\"Q3 - Date-wise unique filename counts:\", q3_datewise_filenames(df))\n",
        "    print(\"Q4 - Number of 404 responses:\", q4_count_404(df))\n",
        "    print(\"Q5 - Top 15 filenames with 404 responses:\", q5_top_15_404_filenames(df))\n",
        "    print(\"Q6 - Top 15 file extensions with 404 responses:\", q6_top_15_404_extensions(df))\n",
        "    print(\"Q7 - Total bandwidth per day (July 1995):\", q7_july_bandwidth(df))\n",
        "    print(\"Q8 - Hourly request distribution:\", q8_hourly_distribution(df))\n",
        "    print(\"Q9 - Top 10 most requested filenames:\", q9_top_10_filenames(df))\n",
        "    print(\"Q10 - HTTP response code distribution:\", q10_http_code_distribution(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ptAK5x1mWhja",
        "outputId": "a20aa024-6cd8-4d51-9fc2-6ced040fdbe1"
      },
      "id": "ptAK5x1mWhja",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading dataset from calgary_access_log.gz...\n",
            "Dataset read successfully.\n",
            "Parsing log entries...\n",
            "Successfully parsed 724837 entries out of 726739.\n",
            "Creating DataFrame...\n",
            "DataFrame created successfully.\n",
            "Converting timestamp column to datetime...\n",
            "Timestamp column conversion complete.\n",
            "Removed 413054 rows with invalid timestamps after DataFrame creation.\n",
            "Extracting date and hour from timestamp...\n",
            "Date and hour columns added.\n",
            "\n",
            "Data loaded and cleaned. DataFrame shape: (311783, 8)\n",
            "DataFrame columns: ['host', 'timestamp', 'filename', 'file_ext', 'http_code', 'bytes', 'date', 'hour']\n",
            "\n",
            "--- Analysis Results ---\n",
            "Q1 - Total log records: 311783\n",
            "Q2 - Unique hosts: 2\n",
            "Calculating date-wise unique filename counts...\n",
            "Calculation complete.\n",
            "Q3 - Date-wise unique filename counts: {'01-Aug-1995': 684, '01-Jul-1995': 388, '01-Jun-1995': 591, '01-May-1995': 467, '01-Sep-1995': 328, '02-Apr-1995': 438, '02-Aug-1995': 857, '02-Jul-1995': 400, '02-Jun-1995': 515, '02-May-1995': 702, '02-Sep-1995': 286, '03-Apr-1995': 795, '03-Aug-1995': 584, '03-Jul-1995': 438, '03-Jun-1995': 401, '03-May-1995': 609, '04-Apr-1995': 821, '04-Aug-1995': 717, '04-Jul-1995': 612, '04-Jun-1995': 371, '04-May-1995': 684, '05-Apr-1995': 891, '05-Aug-1995': 510, '05-Jul-1995': 607, '05-Jun-1995': 495, '05-May-1995': 609, '06-Apr-1995': 679, '06-Aug-1995': 448, '06-Jul-1995': 524, '06-Jun-1995': 670, '06-May-1995': 517, '07-Apr-1995': 777, '07-Aug-1995': 609, '07-Jul-1995': 428, '07-Jun-1995': 487, '07-May-1995': 726, '08-Apr-1995': 547, '08-Aug-1995': 657, '08-Jul-1995': 277, '08-Jun-1995': 651, '08-May-1995': 671, '09-Apr-1995': 627, '09-Aug-1995': 699, '09-Jul-1995': 233, '09-Jun-1995': 468, '09-May-1995': 775, '10-Apr-1995': 752, '10-Aug-1995': 637, '10-Jul-1995': 505, '10-Jun-1995': 328, '10-May-1995': 795, '11-Apr-1995': 818, '11-Aug-1995': 456, '11-Jul-1995': 573, '11-Jun-1995': 299, '11-May-1995': 600, '12-Apr-1995': 887, '12-Aug-1995': 342, '12-Jul-1995': 469, '12-Jun-1995': 521, '12-May-1995': 469, '13-Apr-1995': 619, '13-Aug-1995': 464, '13-Jul-1995': 502, '13-Jun-1995': 467, '13-May-1995': 289, '14-Apr-1995': 362, '14-Aug-1995': 592, '14-Jul-1995': 553, '14-Jun-1995': 590, '14-May-1995': 326, '15-Apr-1995': 422, '15-Aug-1995': 482, '15-Jul-1995': 386, '15-Jun-1995': 479, '15-May-1995': 584, '16-Apr-1995': 438, '16-Aug-1995': 604, '16-Jul-1995': 300, '16-Jun-1995': 532, '16-May-1995': 432, '17-Apr-1995': 446, '17-Aug-1995': 539, '17-Jul-1995': 569, '17-Jun-1995': 386, '17-May-1995': 508, '18-Apr-1995': 455, '18-Aug-1995': 495, '18-Jul-1995': 560, '18-Jun-1995': 360, '18-May-1995': 529, '19-Apr-1995': 701, '19-Aug-1995': 379, '19-Jul-1995': 473, '19-Jun-1995': 618, '19-May-1995': 499, '20-Apr-1995': 587, '20-Aug-1995': 397, '20-Jul-1995': 570, '20-Jun-1995': 531, '20-May-1995': 254, '21-Apr-1995': 713, '21-Aug-1995': 634, '21-Jul-1995': 655, '21-Jun-1995': 625, '21-May-1995': 289, '22-Apr-1995': 435, '22-Aug-1995': 541, '22-Jul-1995': 455, '22-Jun-1995': 634, '22-May-1995': 478, '23-Apr-1995': 333, '23-Aug-1995': 662, '23-Jul-1995': 501, '23-Jun-1995': 566, '23-May-1995': 566, '24-Apr-1995': 529, '24-Aug-1995': 579, '24-Jul-1995': 567, '24-Jun-1995': 401, '24-May-1995': 490, '24-Oct-1994': 231, '25-Apr-1995': 558, '25-Aug-1995': 598, '25-Jul-1995': 592, '25-Jun-1995': 587, '25-May-1995': 489, '25-Oct-1994': 319, '26-Apr-1995': 649, '26-Aug-1995': 398, '26-Jul-1995': 599, '26-Jun-1995': 638, '26-May-1995': 427, '26-Oct-1994': 377, '27-Apr-1995': 616, '27-Aug-1995': 437, '27-Jul-1995': 616, '27-Jun-1995': 518, '27-May-1995': 251, '27-Oct-1994': 385, '28-Apr-1995': 637, '28-Aug-1995': 552, '28-Jul-1995': 567, '28-Jun-1995': 573, '28-May-1995': 205, '28-Oct-1994': 405, '29-Apr-1995': 449, '29-Aug-1995': 514, '29-Jul-1995': 323, '29-Jun-1995': 469, '29-May-1995': 468, '29-Oct-1994': 290, '30-Apr-1995': 281, '30-Aug-1995': 595, '30-Jul-1995': 486, '30-Jun-1995': 461, '30-May-1995': 556, '30-Oct-1994': 20, '31-Aug-1995': 514, '31-Jul-1995': 625, '31-May-1995': 575}\n",
            "Counting 404 responses...\n",
            "Count complete.\n",
            "Q4 - Number of 404 responses: 10723\n",
            "Finding top 15 filenames with 404 responses...\n",
            "Calculation complete.\n",
            "Q5 - Top 15 filenames with 404 responses: [('index.html', 2132), ('4115.html', 697), ('1611.html', 484), ('5698.xbm', 438), ('710.txt', 198), ('6555.html', 152), ('9388.xbm', 120), ('151.html', 107), ('9678.gif', 96), ('394.html', 93), ('9814.gif', 88), ('449.gif', 85), ('1685.html', 82), ('4278.html', 75), ('7362.gif', 72)]\n",
            "Finding top 15 file extensions with 404 responses...\n",
            "Calculation complete.\n",
            "Q6 - Top 15 file extensions with 404 responses: [('html', 6006), ('gif', 3073), ('xbm', 603), ('txt', 217), ('ps', 204), ('jpg', 139), ('htm', 31), ('dvi', 21), ('z', 18), ('util', 17), ('com', 16), ('html&', 15), ('', 14), ('mpg', 13), ('au', 12)]\n",
            "Calculating total bandwidth per day for July 1995...\n",
            "Calculation complete.\n",
            "Q7 - Total bandwidth per day (July 1995): {'01-Jul-1995': 11349799, '02-Jul-1995': 8656918, '03-Jul-1995': 13596612, '04-Jul-1995': 26573988, '05-Jul-1995': 19541225, '06-Jul-1995': 19755015, '07-Jul-1995': 9427822, '08-Jul-1995': 5403491, '09-Jul-1995': 4660556, '10-Jul-1995': 14917754, '11-Jul-1995': 22507207, '12-Jul-1995': 17367065, '13-Jul-1995': 15989234, '14-Jul-1995': 19186430, '15-Jul-1995': 15773233, '16-Jul-1995': 9016378, '17-Jul-1995': 19601338, '18-Jul-1995': 17099761, '19-Jul-1995': 17851725, '20-Jul-1995': 20752623, '21-Jul-1995': 25491617, '22-Jul-1995': 8136259, '23-Jul-1995': 9593870, '24-Jul-1995': 22308265, '25-Jul-1995': 24561635, '26-Jul-1995': 24995540, '27-Jul-1995': 25969995, '28-Jul-1995': 36460693, '29-Jul-1995': 11700624, '30-Jul-1995': 23189598, '31-Jul-1995': 30730715}\n",
            "Calculating hourly request distribution...\n",
            "Calculation complete.\n",
            "Q8 - Hourly request distribution: {0: 9152, 1: 7987, 2: 7797, 3: 6682, 4: 6380, 5: 6744, 6: 7910, 7: 9497, 8: 12622, 9: 14884, 10: 18330, 11: 19626, 12: 18580, 13: 20790, 14: 20751, 15: 19714, 16: 19379, 17: 15747, 18: 12549, 19: 12056, 20: 12041, 21: 11290, 22: 10932, 23: 10343}\n",
            "Finding top 10 most requested filenames...\n",
            "Calculation complete.\n",
            "Q9 - Top 10 most requested filenames: [('index.html', 53462), ('3.gif', 8232), ('2.gif', 7946), ('4097.gif', 3469), ('8870.jpg', 3416), ('6733.gif', 3328), ('244.gif', 3111), ('8472.gif', 2748), ('4.gif', 2596), ('8308.gif', 2500)]\n",
            "Calculating HTTP response code distribution...\n",
            "Calculation complete.\n",
            "Q10 - HTTP response code distribution: {200: 237754, 304: 49717, 302: 12429, 404: 10723, 403: 1066, 401: 39, 500: 28, 501: 20, 400: 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CtziTjscWirg"
      },
      "id": "CtziTjscWirg",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "poetry-x2j_aZzr-py3.12",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}